# MakeWeatherGreatAgain

# Задача

Основной задачей своего анализа, я поставил вопрос: "Действительно ли погода летом 2019 года была хуже чем в предыдущие года".

Причиной по которой я сделал данный анализ послужил информационный фон вокруг меня. Из социальных сетей и новостей я слышал, что "лето стало не такое" и "Раньше было теплее".
Я решил проверить это.

# Решение

## Изначальный план:

1. Создать парсер погодных данных погоды в Москве.
2. Развернуть БД PostgreSQL.
3. Спарсить погодные данные и поместить их в БД PostgreSQL.
4. Провести анализ собранных данных в Jupyter Notebook

## Реальные шаги:
1. Необходимо было найти источник погодных данных. Таковым стал сайт wunderground.com.
2. Под данные на сайте я развернул PostgreSQL и создал необходимые таблицы.
3. Изначально я предполагал стащить данные прямо со страниц сайта где данные отображаются. Но через консоль разработчика выяснил, что у данного сайта есть API к которому идет запрос при переходе пользователя.
4. Далее я создал приложение которое использую TOR подключается к API wunderground.com и получает данные за необходимые года.
5. Данные с wouderground.com я поместил в БД.
6. Далее я подключился через консоль к БД и начал строить простейшие запросы, чтобы выяснить все ли данные успешно выгрузились. И когда я построил количество дней по годам выяснилось, что дней по годам меньше чем должно быть.
7. Чтобы найти каких данных не хватает я строил запросы к тем годам в которых не хватало дней, сначала по месяцам, затем по дням. Когда я нашел отсуствующие дни, то решил посмотреть есть ли эти данные на самом сайте. Оказалось, что там они также отсуствовали.
8. Я начал искать сайт на котором я еще могу получить погодные данные. Таким сайтом оказался rp5.ru. Проблема была в том, что с сайта можно скачивать файлы по месяцам в ручную, а уже дальше работать с ними как с CSV файлами.
9. Для того, чтобы получить данные я создал парсер при помощи Selenium. Данный парсер иммитировал работу пользователя. Он переходит на страницу, выбирает нужный период выгрузки и жмет на кнопку скачать.
10. Количество скаченных файлов соответствовало 15 годам * 12 месяцев = 180 файлов. Для обработки этих файлов я написал приложение которое открывало каждый файл, брало необходимые данные и помещало их в БД. 
11. Я проверил SQL запросами, что данные за все дни присуствуют.
12. Далее анализ я проводил в файле Jupyter Notebook.

## Файлы:
- analys.ipynb - анализ собранных данных в Jupyter Notebook
- analys.py - анализ собранных данных кодом python
- analys.sql - анализ собранных данных в SQL
- files.py - модуль по работе с файлами (json, csv)
- init.sq - файл который помог в создании таблиц PostgreSQL
- main.py - основной файл с запуском меню для работы программы
- README.md - файл для вывода информации которую сейчас читаете
- scraping.py - модуль по скрапингу сайтов с помощью Selenium и API
- sql.py - модуль для работы с PostgreSQL (Чтение, добавление данных) 